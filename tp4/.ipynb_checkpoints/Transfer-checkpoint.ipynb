{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning pour la classificaion cats vs. dogs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import adam\n",
    "from keras import applications\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cahier, nous utiliserons des images étiquetées de chats et de chiens pour former un classifieur capable de les distinguer. Comme beaucoup d’autres dans le domaine de la vision par ordinateur, c’est une tâche que nous avons appris à maîtriser au cours des dernières années, principalement grâce aux réseaux de neurones à convolution.\n",
    "Il y a eu un concours sur Kaggle pour cela en $2014$. Pierre Sermanet, élève de Yann LeCun, a pris la 1ère place, avec une impressionnante précision de 98,9% sur le set de test. Il explique brièvement comment il l'a fait dans cet article Google+:\n",
    "\n",
    "\"*Je viens de gagner le concours Dogs vs. Cats Kaggle, en utilisant la bibliothèque d'apprentissage en profondeur que j'avais écrite lors de ma thèse: OverFeat http://cilvr.nyu.edu/doku.php?id=code:start\n",
    "Mon système a été pré-formé sur ImageNet (ensemble de données de classification ILSVRC12) et ensuite affiné sur les données de chats et de chiens. J'ai limité mon nombre de soumissions à 5 pour éviter le réglage des ensembles de tests et j'ai obtenu la 1ère place contre 215 autres équipes avec une précision de 98,9%.*\"\n",
    "\n",
    "Mais qu'est-ce que cela signifie exactement de pré-former un modèle? Eh bien, découvrons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser les données du concours Kaggle. Le jeu de données est assez volumineux: il existe 25 000 échantillons d’entraînement et 1 200 échantillons d’essai, tous de tailles différentes. Nous allons donc travailler avec seulement quelques-uns d'entre eux: nous prenons 3000 échantillons du kit de formation, et nous en utilisons 2000 pour la formation et 1000 pour la validation.\n",
    "\n",
    "**WARNING**: telechargez le  dataset plus peti ici [here](https://www.dropbox.com/s/teiafmdpt49pddd/cats_and_dogs_small.zip?dl=0) puis un-zipez le dans un dossier `mldata`.\n",
    "\n",
    "Chargeons les données - Keras a quelques bonnes routines pour cela. Nous commençons par charger une seule image, juste pour voir à quoi ça ressemble (essayez de changer le nom du fichier pour en voir d'autres). Notez que nous faisons des images en couleurs et que nous travaillons donc avec des tableaux à 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and transform it to a Numpy array\n",
    "img = load_img(\"mldata/cats_and_dogs_small/train/cats/cat.2.jpg\")\n",
    "x = img_to_array(img)\n",
    "print(\"array size:\", x.shape)\n",
    "\n",
    "# Show image\n",
    "plt.imshow(x / 255.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous spécifions quelques paramètres de formation et créons un * générateur * pour les données, en utilisant un outil très pratique de Keras appelé `ImageDataGenerator`. Essentiellement, il parcourt les images du répertoire et les traite à notre intention.\n",
    "\n",
    "Ci-dessous, nous redimensionnons chacune des images afin que chaque intensité de pixel passe de 0 à 1 au lieu de 0 à 255; et également ajouter du bruit à l'image pour que le classificateur devienne plus robuste, une technique connue sous le nom de *augmentation de données*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "\n",
    "n_train_samples = 2000\n",
    "n_valid_samples = 1000\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# Set up generator for training and validation images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_generator = train_datagen.flow_from_directory(\"mldata/cats_and_dogs_small/train\",\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"binary\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_generator = test_datagen.flow_from_directory(\"mldata/cats_and_dogs_small/valid\",\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un CNN de zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous allons écrire notre réseau de neurones en utilisant Keras et le former. Ci-dessous, nous utilisons un réseau de neurones avec 3 couches convolutives censées apprendre les caractéristiques pertinentes et 2 couches plus denses qui utiliseront les caractéristiques apprises pour classer l'image en tant que chat ou chien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify network architecture: 3 conv. layers w/ ReLU activations + 2 dense layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut maintenant faire l'entrainement, ce qui prend un peu de temps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model\n",
    "#model.fit_generator(\n",
    "    #train_generator,\n",
    "    #steps_per_epoch=n_train_samples // batch_size,\n",
    "    #epochs=epochs,\n",
    "    #validation_data=valid_generator,\n",
    "    #validation_steps=n_valid_samples // batch_size)\n",
    "\n",
    "#model.save_weights(\"mldata/cnn_catsvsdogs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (or if you don't want to wait you can't just load the weights below)\n",
    "model.load_weights(\"mldata/cnn_catsvsdogs.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouf, ça a pris longtemps (sauf si vous avez un GPU!). Si seulement nous pouvions utiliser des poids que nous avons déjà formés sur d'autres jeux de données ...\n",
    "\n",
    "Voyons quelle est la précision obtenue sur l'ensemble de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate_generator(valid_generator, steps=n_valid_samples//batch_size)\n",
    "print(\"accuracy on test set:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas mal - avec une telle précision, nous serions parmi la moitié supérieure du concours Kaggle (voir réf. 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untiliser un model pre-entrainé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formation de notre CNN a pris beaucoup de temps. Et si nous le remplaions par un autre, formé dans un * jeu de données similaire *? Après tout, les caractéristiques importantes devraient être plus ou moins les mêmes, non?\n",
    "\n",
    "Les gens ont utilisé beaucoup de ressources informatiques, formant des réseaux très profonds sur de vastes ensembles de données. Heureusement, ils ont mis leurs poids à disposition afin que nous puissions les utiliser! Ci-dessous, nous allons utiliser un réseau de neurones convolutifs à 16 couches formé sur le jeu de données Imagenet, connu sous le nom de [VGG-16] (http://www.robots.ox.ac.uk/%7Evgg/research/very_deep/).\n",
    "\n",
    "![VGG16](vgg16.png)\n",
    "\n",
    "Faisons-le par étapes: nous chargeons d'abord le réseau VGG16, sans la dernière couche de classificateur(`include_top=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 weights\n",
    "vgg16 = applications.VGG16(include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous créons les générateurs comme auparavant (maintenant sans aucune augmentation de données) et passons l'ensemble de nos échantillons via le réseau VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = vgg16.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            print(f\"{i * batch_size} samples generated for {directory}\")\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes take a while...\n",
    "n_train_samples = 1000\n",
    "n_valid_samples = 500\n",
    "\n",
    "train_features, train_labels = extract_features(\"mldata/cats_and_dogs_small/train\", n_train_samples)\n",
    "valid_features, valid_labels = extract_features(\"mldata/cats_and_dogs_small/valid\", n_valid_samples)\n",
    "print(f\"train_features: {train_features.shape}\")\n",
    "print(f\"valid_features: {valid_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de comprendre ce que fait le réseau VGG16. Voyons d'abord à quoi ressemble la sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 8, figsize=(7, 7))\n",
    "for i in range(64):\n",
    "    axs[i // 8, i % 8].imshow(train_features[0, :, :, i], cmap=\"gray\")\n",
    "    axs[i // 8, i % 8].get_xaxis().set_visible(False)\n",
    "    axs[i // 8, i % 8].get_yaxis().set_visible(False)\n",
    "fig.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il consiste en 512 images de taille 4x4, mais les regarder ne nous révèle pas grand-chose ... Regardons la sortie des filtres un par un."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that reads image and returns each layer output\n",
    "input_img = vgg16.input\n",
    "outputs = [layer.output for layer in vgg16.layers][1:]\n",
    "functors = [K.function([input_img], [out]) for out in outputs]\n",
    "\n",
    "# Input first image in the training set to this function\n",
    "img_generator = datagen.flow_from_directory(\n",
    "    \"mldata/cats_and_dogs_small/train\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "img_generator.reset()\n",
    "img = img_generator.next()\n",
    "\n",
    "layer_outputs = [func([img])[0] for func in functors]\n",
    "for i in range(len(layer_outputs)):\n",
    "    print(\"layer %d shape: %s\" % (i, layer_outputs[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changez la valeur de `layer` ci-dessous, de 0 à 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "\n",
    "fig, axs = plt.subplots(8, 8, figsize=(14, 14))\n",
    "for i in range(64):\n",
    "    axs[i // 8, i % 8].imshow(layer_outputs[layer][0, :, :, i], cmap=\"gray\")\n",
    "    axs[i // 8, i % 8].get_xaxis().set_visible(False)\n",
    "    axs[i // 8, i % 8].get_yaxis().set_visible(False)\n",
    "fig.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons comprendre chaque couche d'un CNN comme effectuant plusieurs tâches de traitement d'image en parallèle - détection des contours, netteté, flou - [en effectuant des convolutions avec les filtres appris] (https://en.wikipedia.org/wiki/Kernel_ (traitement_image) ). Ces tâches ne sont pas effectuées sur l'image d'origine, mais sur la sortie du calque précédent. c'est pourquoi la sortie des couches les plus à droite devient très difficile à interpréter.\n",
    "\n",
    "La tâche à effectuer dépend du filtre utilisé. Nous utilisons ici les filtres intégrés à VGG16, mais nous pourrions en principe les apprendre, si nous formions un CNN à partir de rien.\n",
    "\n",
    "Un exercice intéressant, qui fournit un moyen d’interpréter la sortie des couches les plus à droite, consiste à trouver l’image [qui maximise l’activation d’un filtre donné] (https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html).\n",
    "\n",
    "Enfin, nous prenons la sortie du réseau VGG16 et la connectons au classifieur à 2 couches que nous avions auparavant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import rmsprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_features.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=rmsprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainons-le !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs = 20,\n",
    "    batch_size = 16,\n",
    "    validation_data = (valid_features, valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(valid_features, valid_labels)\n",
    "print(\"accuracy on test set:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça a l'air bien non? Et certainement beaucoup plus vite :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 8 images at random, pass them through VGG16 and then through classifier\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "generator = datagen.flow_from_directory(\"mldata/cats_and_dogs_small/valid\",\n",
    "                         target_size=(img_width, img_height),\n",
    "                         batch_size=8,\n",
    "                         class_mode=\"binary\")\n",
    "batch = generator.next()\n",
    "features = vgg16.predict(batch[0])\n",
    "probs = model.predict_proba(features)\n",
    "\n",
    "# Show images together with probabilities\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(8):\n",
    "    axs[i // 4, i % 4].imshow(batch[0][i])\n",
    "    axs[i // 4, i % 4].set_title(\"prob. dog: %.2f\" % (probs[i]))\n",
    "    \n",
    "    axs[i // 4, i % 4].get_xaxis().set_visible(False)\n",
    "    axs[i // 4, i % 4].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "2. https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "3. https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069\n",
    "4. https://github.com/abursuc/dldiy-practicals/blob/master/10_05_lesson1.ipynb\n",
    "5. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/dogs_cats_redux.ipynb\n",
    "6. http://www.cs.toronto.edu/~frossard/post/vgg16/\n",
    "7. https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
