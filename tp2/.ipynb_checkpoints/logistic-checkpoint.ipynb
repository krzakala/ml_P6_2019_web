{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression logistique à partir de zéro!\n",
    "\n",
    "Notre but est le suivant: nous allong créer des données synthetiques bidimensionnelles qui sont linearement séparables, puis nous allons écrire un algorithme de régression logistique. C'est un tres bon, et tres instructif, exercice de python!\n",
    "\n",
    "Commencons par creer des points en dimension 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 500\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .5],[.5, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .8],[.8, 1]], num_observations)\n",
    "\n",
    "dataset = np.vstack((x1, x2)).astype(np.float32)\n",
    "labels = np.hstack((np.zeros(num_observations),np.ones(num_observations)))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1],c = labels, alpha = .4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre objectif est d'utiliser une modélisation de type logistique ici. Dans ce cas:\n",
    "$$P_{\\rm model}(l({\\bf x})=1) = \\frac 1{1+\\exp(-{\\bf \\theta} \\cdot {\\bf x})}~~~ \\text{&}~~~ P_{\\rm model}(l({\\bf x})=0) = \\frac {\\exp(-{\\bf \\theta} \\cdot {\\bf x})}{1+\\exp(-{\\bf \\theta} \\cdot {\\bf x})}$$\n",
    "\n",
    "Maintenant, nous utilisons comme fonction de perte l'entropie croisée, et nous écrivons:\n",
    "$${\\rm Loss} = - \\sum_{\\rm dataset} \\sum_{l=0,1} P_{\\rm true}(x=l) \\log(P_{\\rm model}(x=l)) $$\n",
    "\n",
    "Verifiez que cela donne:\n",
    "$${\\rm Loss} =  \\sum_{\\rm dataset} - y_i {\\bf \\theta} \\cdot {\\bf x}_i  + \\log{(1+\\exp({\\bf \\theta} \\cdot {\\bf x}_i ))} $$\n",
    "\n",
    "Cette expression est un peu differente de celle. vue. en cours, mais elle est tout a fait equivalente. Il nous faut maintenant implementer cette fonction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "def log_loss(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = #WRITE HERE THE EXPRESSION OF THE LOSS\n",
    "    return -ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de réaliser l'optimisation, nous devons calculer le gradient et effectuer une montée de gradient. Ici nous avons:\n",
    "\n",
    "$$\\nabla {\\rm Loss} =  \\sum_{i \\in \\rm dataset} - y_i  {\\bf x}_i  + {\\bf x}_i  \\frac{\\exp({\\bf \\theta} \\cdot {\\bf x}_i )}{(1+\\exp({\\bf \\theta} \\cdot {\\bf x}_i ))} = - \\sum_{i \\in \\rm dataset} {\\bf x^T}_i (y_i - P(l({\\bf x_i})=1)) $$\n",
    "\n",
    "Nous pouvons maintenant écrire la régression logistique et effectuer la descenet de gradient en ecrivant:\n",
    "\n",
    "$$\n",
    "{\\bf \\theta}^{t+1} = {\\bf \\theta}^{t} - \\eta \\nabla {\\rm Loss} \n",
    "$$\n",
    "ou $\\eta$ est  appelle le \"learning rate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features, target, num_steps, learning_rate):\n",
    "#logistic regression with leanring rate learning_rate for a number of steps num_steps\n",
    "#target is the true set of labels, and features is the matrix of data n * d\n",
    "\n",
    "    weights = np.zeros(features.shape[1])##Initialization from 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with gradient\n",
    "        output_error_signal = ### IMPLENTEZ ICI ????????????\n",
    "        gradient = ### IMPLENTEZ ICI ????????\n",
    "        weights += ### IMPLENTEZ ICI ???????\n",
    "        \n",
    "        # Print log-likelihood from time to time\n",
    "        if step % 10000 == 0:\n",
    "            print (step,\" \",log_loss(features, target, weights))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons l’astuce habituelle qui consiste à ajouter un \"1\" aux données pour pouvoir effectuer un apprentissage avec une fonction afine (ax+b) plutot que simplement lineare (ax), et nous appelons la fonction de régression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intercept = np.ones((dataset.shape[0], 1))\n",
    "data_with_intercept = np.hstack((intercept, dataset))\n",
    "\n",
    "weights = logistic_regression(data_with_intercept, labels, num_steps = 300000, learning_rate = 5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant tracer les prévisions à partir de notre modèle et vérifier leur qualité dans le jeu de données d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(x,a,b,c):\n",
    "    return -x*b/c-a/c\n",
    "def myline(x):\n",
    "    return line(x,weights[0],weights[1],weights[2])\n",
    "\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "print('Accuracy: {0}'.format((preds == labels).sum().astype(float) / len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1],\n",
    "            c = (preds == labels) , alpha = .8, s = 50)\n",
    "plt.plot([-3,4],[myline(-3),myline(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que se passe t-il si le num_steps est trop petit? Ou si le learning_rate est trop grand? "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
